\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Principles for modelling packages},
            pdfauthor={TBD},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\newcommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Principles for modelling packages}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{TBD}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2018-08-08}


\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\chapter{Intro}\label{intro}

\textbf{Rule 1}: Always spell it \emph{modelling}, never
\emph{modeling}.

This document is targetted at R developers writing new packages for
modelling.

For users interesting in learning more about using R for modelling in
practice, we recommend X and Y

\chapter{Conceptual overview of
modelling}\label{conceptual-overview-of-modelling}

\begin{itemize}
\item
  what is a model: models, estimands, estimators and model
  specifications
\item
  what do we do with models
\item
  how do fit models
\item
  once we have a fit model, how do we predict or do inference
\item
  the difference between working with a single fit vs a set of fits.
  LASSO example: wanting to use the coefficients for prediction vs
  wanting to see the order in which features enter the model
\end{itemize}

\chapter{Getting started on a modelling
package}\label{getting-started-on-a-modelling-package}

General dos:

\begin{itemize}
\tightlist
\item
  Export the \texttt{predict()} method
\item
  Document the \texttt{predict()} method
\item
  Use \texttt{match.arg()} for categorical arguments
\item
  Validate the arguments to all your functions, especially our data
\end{itemize}

General dont's

\begin{itemize}
\item
\end{itemize}

\chapter{Model objects}\label{model-objects}

\begin{itemize}
\item
  some explanation of why and how to save the function call
\item
  generally what kinds of things should go into a model object, giving
  model objects a class so other people can extend them
\item
  S3 object creation and validation for model building a la Advanced R
\item
  Model classes beyond lists. when is S4 worth it? when is R6?
\item
  Every modeling function should include its package version in its data
  object I will now save my models as a list of three objects: model,
  data, and sessioninfo::session\_info()
\end{itemize}

\chapter{Data Specification}\label{data-specification}

\begin{itemize}
\item
  formulas, model.frame, term objects, etc
\item
  data / design matrix specification - \texttt{recipes}
\end{itemize}

habit: get the df right, then y \textasciitilde{} . in the formula.
would be nice to still see the features in the call?

\begin{itemize}
\tightlist
\item
  ask users to use data.frames and tibbles, not matrices.
\end{itemize}

\section{Formulas}\label{formulas}

\subsection{Testing formulas}\label{testing-formulas}

\url{https://github.com/alexpghayes/formulize/blob/master/tests/testthat/test_formula.R}

minimum set of formula tests (based on mtcars example dataset):

\begin{itemize}
\tightlist
\item
  using \texttt{as.factor()} inline
  \texttt{mpg\ \textasciitilde{}\ as.factor(hp)}
\item
  using \texttt{as.character()} inline
  \texttt{mpg\ \textasciitilde{}\ as.character(hp)}
\item
  intercept only \texttt{mpg\ \textasciitilde{}\ 1}
\item
  no intercept
  \texttt{mpg\ \textasciitilde{}\ disp\ +\ hp\ +\ drat\ -\ 1}
\item
  implicit intercept
  \texttt{mpg\ \textasciitilde{}\ disp\ +\ hp\ +\ drat}
\item
  explicit intercept
  \texttt{mpg\ \textasciitilde{}\ disp\ +\ hp\ +\ drat\ +\ \ 1}
\item
  polynomials with 1 term
  \texttt{mpg\ \textasciitilde{}\ disp\ +\ hp\ +\ poly(drat,\ 1)}
\item
  polynomials with multiple terms
  \texttt{mpg\ \textasciitilde{}\ disp\ +\ hp\ +\ poly(drat,\ 3)}
\item
  natural splines with 1 term
  \texttt{mpg\ \textasciitilde{}\ disp\ +\ hp\ +\ ns(drat,\ 1)}
\item
  natural splines with multiple terms
  \texttt{mpg\ \textasciitilde{}\ disp\ +\ hp\ +\ ns(drat,\ 3)}
\item
  explicit interactios
  \texttt{mpg\ \textasciitilde{}\ drat\ +\ hp\ +\ drat:hp}
\item
  dot \texttt{mpg\ \textasciitilde{}\ .}
\item
  star \texttt{mpg\ \textasciitilde{}\ hp\ *\ drat}
\item
  as.is \texttt{mpg\ \textasciitilde{}\ hp\ +\ I(drat\^{}2)}
\item
  multiple response cbind
\item
  multiple responses as matrix \texttt{y\ \textasciitilde{}\ x} where
  \texttt{y} is a matrix
\item
  multiple predictors as matrix \texttt{y\ \textasciitilde{}\ x} where
  \texttt{x} is a matrix
\item
  multiple predictos and responses together
  \texttt{y\ \textasciitilde{}\ x}, both \texttt{x}, \texttt{y} matrices
\item
  transformed response \texttt{log(mpg)\ \textasciitilde{}\ hp\ +\ drat}
\end{itemize}

optional / to: - \texttt{survival::Surv} and \texttt{survival::strata}
objects

\chapter{Functional programming
principles}\label{functional-programming-principles}

calls to fit should be pure: i.e.~no side effects like plotting, and
especially no plotting with invisible object return - side effects:
useful in interactive mode, irritating in programmatic mode

\begin{itemize}
\tightlist
\item
  type safety, particularly of returned objects
\item
  type safety with respect to single fits vs sets of fits
\end{itemize}

\chapter{Data}\label{data}

specification, exported data, and data sets used internally in a package

\begin{itemize}
\tightlist
\item
  using data from the package in tests
\item
  using data from \emph{other} packages in tests
\end{itemize}

\chapter{Documentation}\label{documentation}

\begin{itemize}
\item
  vignette should include not only the coefficients as output in an
  example, but also those coefficients written up as a general latex
  model and as a latex model with those specific coefficients
  substituted in
\item
  \textbf{show} your example data in the README so users immediately see
  the structure
\end{itemize}

function to write out model form and fitted model in latex for sanity
checking: some sort of model\_report / model\_form generic. think
\texttt{report} generic or \texttt{write.model} may be coming to
\texttt{fable}/\texttt{forecast} soon.

it's a bad idea to expect users to learn the \emph{math} for your model
from function level documentation, or math presented in ascii or unicode
or poorly rendered latex.

show write out the math in a nicely formatted vignette, and then clearly
describe the connection between code objects and math objects there as
well

documenting arguments:

\begin{itemize}
\tightlist
\item
  \texttt{data}: super important to document acceptable \textbf{types}
  and formats, and highly recommend provided a dataset in \texttt{data/}
  with this format so the user can see exactly what they need to
  provide.
\end{itemize}

bad doc: The dataset to fit on the model on better doc: A data.frame or
tibble with one row per observation and one column per features. For
example, \texttt{mtcars} is in this format, but \texttt{messy\_data} is
not. It is okay to specify a matrix so long as it can coerced to a
tibble. etc etc

\chapter{Testing}\label{testing}

\begin{itemize}
\tightlist
\item
  testing against existing software - say a Matlab implementation
\item
  saving long running models in \texttt{R/sysdata.rda} with
  \texttt{usethis::use\_data(model\_obj,\ internal\ =\ TRUE)}
\end{itemize}

\chapter{Workflow}\label{workflow}

\section{Prediction}\label{prediction}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  feature engineering
\item
  ML wizardry
\item
  more feature engineering
\item
  ???
\item
  predictions
\end{enumerate}

\section{Inference}\label{inference}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Clean data
\item
  Specify model
\item
  Fit model
\item
  Check that model fitting process converged / worked
\item
  Check statistical assumptions of model
\end{enumerate}

KEY part that always gets left out: working with multiple modellings

\chapter{Interface}\label{interface}

\begin{itemize}
\tightlist
\item
  user friendly interfaces
\end{itemize}

good and bad existing idioms

\begin{itemize}
\tightlist
\item
  methods to implement
\item
  examples of tried and true workflows
\end{itemize}

methods to implement - note on plotting: Should be easy to get the
values plotted so others can make their own plots

TWO DISTINCT ISSUES THAT GET RESOLVED IN FORMULAE:

\begin{verbatim}
design matrix specification
model specification. (a la fGarch::garchFit(~arma(1, 1) + garch(1, 0)))
\end{verbatim}

\chapter{Low and high level
interfaces}\label{low-and-high-level-interfaces}

\begin{itemize}
\tightlist
\item
  high level versus low level interface
\item
  programmatic versus interactive use
\end{itemize}

when you should use which

examples: - high level: keras, brms - low level: tensorflow, stan

\chapter{Interactive modelling}\label{interactive-modelling}

what most people do different because there's a person looking at stuff
as opposed to programmatic model when it's just code interacting with
the model with no human involved

this is a chapter mostly to remind us to think of differences between
the two and how they might be important in terms of interface

\chapter{Programmatic modelling}\label{programmatic-modelling}

i.e.~interacting with models programmatically

examples: - packages that export a model from someone to use a la
\href{https://github.com/mkearney/tweetbotornot}{\texttt{botornot}} -
models sitting behind a Plumber API - etc

\chapter{Vocabulary}\label{vocabulary}

useful functions that all modelling package developers should be aware
of

all.names(terms\_object) all.vars(terms\_object)

\section{model frame stuff}\label{model-frame-stuff}

model.frame mode

\section{na.action stuff}\label{na.action-stuff}

\section{quoting operators}\label{quoting-operators}

\chapter{Naming things}\label{naming-things}

\section{How to name function
arguments}\label{how-to-name-function-arguments}

\section{How to name model
components}\label{how-to-name-model-components}

some standard names

currently lots of work happening in this realm in \texttt{broom} and the
Stan community

\url{https://github.com/tidymodels/broom/issues/452}

\chapter{Danger Zone}\label{danger-zone}

little things to include somewhere: - the danger of misspecified
arguments disappearing into \texttt{...}

don't give your model \texttt{lm} class and count on other stuff to
magically work. your should implement methods specifically for your
class, and if that is just wrapping lm internals, great, but take
explicit control

provide an example of extensive formula tests: include a log term in
tests to catch formula edge cases

\section{Warnings / places to use
care}\label{warnings-places-to-use-care}

model.frame() is not the be all end all

double evaluation issue in

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fit <-}\StringTok{ }\KeywordTok{lm}\NormalTok{(hp }\OperatorTok{~}\StringTok{ }\KeywordTok{log}\NormalTok{(mpg), mtcars)}
\KeywordTok{predict}\NormalTok{(fit, }\DataTypeTok{newdata =} \KeywordTok{model.frame}\NormalTok{(fit))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Error in eval(predvars, data, env): object 'mpg' not found
\end{verbatim}

don't do predict(object, newdata = model.frame(object)) since this will
breaaaaaaaak

\section{Anti-patterns}\label{anti-patterns}

\subsection{Using the default method of a
generic}\label{using-the-default-method-of-a-generic}

i.e.~funneling everything into \texttt{confint.default}. Default methods
for new generics should throw an error.

What do to about existing generics?

Key principle here: want to \emph{guarantee} to the user that they are
getting the right numbers

don't funnel everything through augment\_columns and add special cases
slowly - v hard to maintain. much better to have individualized S3
methods with \emph{consistent behavior} abstracted out into small
helpers.

related idea to the belong: have enough classes! using inheritance
appropriately.

\subsection{the documentation that isn't documentation and doesn't
feature an actual use
case}\label{the-documentation-that-isnt-documentation-and-doesnt-feature-an-actual-use-case}

\begin{itemize}
\tightlist
\item
  \emph{cough} \texttt{?MASS::predict.rlm} \emph{cough}
\item
  misisng doc \texttt{?MASS:::predict.polr()}
\end{itemize}

\subsection{Never use missing
arguments}\label{never-use-missing-arguments}

because people have to write more code to pass the objects they want

predict(m) and predict(m, newdata = NULL) should do the same thing you
should test this

\subsection{special casing everything through one workhorse function
instead of using S3
methods}\label{special-casing-everything-through-one-workhorse-function-instead-of-using-s3-methods}

basically \texttt{augment\_columns}

don't funnel everything through augment\_columns and add special cases
slowly - v hard to maintain. much better to have individualized S3
methods with \emph{consistent behavior} abstracted out into small
helpers.

\section{Things to be aware of}\label{things-to-be-aware-of}

If you call things \texttt{data} or \texttt{df} and users fail to
specify data arguments, R will try and perform dataframe operations on
the \emph{functions} \texttt{data} and \texttt{df}. The resulting error
messages for this can be cryptic. In this case you may wish to write an
informative error message with a hint:

\begin{verbatim}
Error: Can't *do data frame thing* on a function.
Are you sure you passed a tibble to *argument*?
\end{verbatim}

\chapter{References}\label{references}

\begin{itemize}
\tightlist
\item
  bdr's
  \href{https://developer.r-project.org/model-fitting-functions.html}{model
  fitting functions in r}
\end{itemize}

\bibliography{book.bib,packages.bib}


\end{document}
